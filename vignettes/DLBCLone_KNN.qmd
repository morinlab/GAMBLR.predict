---
title: "DLBCLone Functions — K-Nearest Neighbors"
vignette: >
  %\VignetteIndexEntry{quarto vignettes}
  %\VignetteEngine{quarto::html}
  %\VignetteEncoding{UTF-8}
from: markdown+emoji
fig.align: "center"
---

To show that our approach also works in the original high-dimensional feature space, we provide two additional functions: `DLBCLone_KNN` and `DLBCLone_KNN_predict`. Unlike the earlier functions, these do not rely on UMAP embeddings. Instead, they call UMAP only to compute pairwise distances in the high-dimensional space, a preliminary step in the UMAP algorithm before applying KNN directly. It’s important not to confuse these with the previously introduced DLBCLone functions, which embed the data in a lower-dimensional space and run KNN on those embeddings. With the earlier functions, users never need to handle the KNN step themselves, since it is built into the workflow.

```{r set working directory and libs, echo=FALSE, include=FALSE, cahce=FALSE}
library(tidyverse)
library(GAMBLR.predict)

dlbcl_meta = readr::read_tsv(system.file("extdata/dlbcl_meta_with_dlbclass.tsv",package = "GAMBLR.predict"))

dlbcl_meta_clean = filter(dlbcl_meta,
                          lymphgen %in% c("MCD","EZB","BN2","N1","ST2","Other"))

best_opt_model_genes <- sort(c(
  "ACTB", "ACTG1", "BCL10", "BCL2", "BCL2L1", 
  "BCL6", "BIRC3", "BRAF", "BTG1", "BTG2", 
  "BTK", "CD19", "CD70", "CD79B", "CD83", 
  "CDKN2A", "CREBBP", "DDX3X", "DTX1", "DUSP2", 
  "EDRF1", "EIF4A2", "EP300", "ETS1", "ETV6", 
  "EZH2", "FAS", "FCGR2B", "FOXC1", "FOXO1", 
  "GNA13", "GRHPR", "HLA-A", "HLA-B", "HNRNPD", 
  "IRF4", "IRF8", "ITPKB", "JUNB", "KLF2", 
  "KLHL14", "KLHL6", "KMT2D", "MEF2B", "MPEG1", 
  "MYD88", "NFKBIA", "NFKBIE", "NFKBIZ", "NOL9", 
  "NOTCH1", "NOTCH2", "OSBPL10", "PIM1", "PIM2", 
  "PRDM1", "PRDM15", "PRKDC", "PRRC2C", "PTPN1", 
  "RFTN1", "S1PR2", "SETD1B", "SGK1", "SOCS1", 
  "SPEN", "STAT3", "STAT6", "TBCC", "TBL1XR1", 
  "TET2", "TMEM30A", "TMSB4X", "TNFAIP3", "TNFRSF14", 
  "TOX", "TP53", "TP73", "UBE2A", "WEE1", 
  "XBP1", "ZFP36L1", "MYD88HOTSPOT", "BCL2_SV", "BCL6_SV"
))

best_opt_model_matrix <- readr::read_tsv(system.file("extdata/all_full_status.tsv",package = "GAMBLR.predict")) %>%
  tibble::column_to_rownames("sample_id") %>%
  select(best_opt_model_genes)
```

# DLBCLone_KNN 

Weighted KNN on a feature (mutation) matrix with optional upweighting of user-specified "core" features, optional exclusion of "hidden" features, and optional optimization of an explicit outgroup (e.g. "Other").

`features_df` matrix (rows = samples, cols = features) rownames must be sample IDs

`metadata` data frame with at least sample_id and the ground-truth label column given in truth_column

`core_features` character vector of feature names to upweight (optional)

`core_feature_multiplier` numeric multiplier for core_features (default: 1.5)

`hidden_features` vector of feature names to drop 

`min_k` integer K range to explore when optimizing (default: 5)

`max_k` integer K range to explore when optimizing (default: 60)

`truth_column` name of metadata column with ground-truth class labels

`truth_classes` vector of all classes to consider (including other_class if you intend to optimize for it)

`other_class` name of the explicit outgroup class (default: "Other")

`optimize_for_other` if TRUE: computes a separate "other" score (ratio) and searches a purity threshold; if FALSE, treats all classes symmetrically

`predict_unlabeled` if TRUE: re-runs KNN to classify samples that were present in features_df but not in metadata

`plot_samples` optional vector of sample_ids to keep in example plots

`DLBCLone_KNN_out` optional prior result; if supplied, its learned parameters are reused (skip optimization)

`epsilon` small value added to distances before weighting

`weighted_votes` If FALSE: neighbors are unweighted (equal votes)

`skip_umap` If TRUE: skip layout optimization plots at the end

```{r, message=FALSE, warning=FALSE}
dlbcl_knn <- DLBCLone_KNN(
  features_df = best_opt_model_matrix,
  metadata = dlbcl_meta_clean,
  min_k = 5,
  max_k = 21,
  optimize_for_other = TRUE
)

knitr::kable(head(dlbcl_knn$predictions))
```
```{r}
dlbcl_knn$plot_truth  
```
```{r}
dlbcl_knn$plot_predicted
```

# DLBCLone_KNN_predict 

Applies a previously optimized `DLBCLone_KNN` model to predict class labels for new (test) samples. This function combines the training and test feature matrices, ensures feature compatibility, and uses the parameters from a DLBCLone KNN optimization run to classify the test samples. Optionally, runs in iterative mode for more stable results when predicting multiple samples. DLBCLone_KNN_predict is only reproducible when you run a sample through individually because the other samples affect the nearest-neighbor distances. It is OK for when users want to “try out” a model but it doesn’t scale well, `predict_single_sample_DLBCLone` is the way to go.

`train_df` matrix of features for training samples (rows = samples, columns = features)

`test_df` matrix of features for test samples to be classified

`metadata` data frame with metadata for all samples, including at least a sample_id column

`core_features` optional character vector of feature names to upweight in the KNN calculation

`core_feature_multiplier` multiplier to apply to core features (default: 1.5)

`hidden_features` optional character vector of feature names to exclude from the analysis

`DLBCLone_KNN_out` output from a previous call to DLBCLone_KNN containing optimized parameters

`mode` if "iterative", runs KNN prediction for each test sample individually (recommended for stability)

## Batch Mode 
```{r, message=FALSE, warning=FALSE, eval=FALSE}
predictions_b <- DLBCLone_KNN_predict(
  train_df = best_opt_model_matrix,
  test_df = valid_df,
  metadata = dlbcl_meta_clean,
  DLBCLone_KNN_out = dlbcl_knn,
  mode = "batch"
)
```
